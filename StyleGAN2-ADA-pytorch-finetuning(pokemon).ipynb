{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StyleGAN2-ADA-pytorch.ipynb","provenance":[],"authorship_tag":"ABX9TyOFj46dbHb6GXelUwDzEJNw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fb_t1N3jA5Fs","executionInfo":{"status":"ok","timestamp":1658974508140,"user_tz":-540,"elapsed":9,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"bec63b1d-5db3-4517-99e2-85ab45b8cb85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul 28 02:15:07 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V46RruSTA9al","executionInfo":{"status":"ok","timestamp":1658974569949,"user_tz":-540,"elapsed":18400,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"61000f1e-7df6-45bc-a66d-11a8063e02b9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/StyleGAN2-ADA-pytorch\n","# !git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n","# !mkdir downloads\n","# !mkdir datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CWvL5jUJBRhj","executionInfo":{"status":"ok","timestamp":1658974758524,"user_tz":-540,"elapsed":5,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"3c982975-e440-4b11-9a74-af26be8520e3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/StyleGAN2-ADA-pytorch\n"]}]},{"cell_type":"markdown","source":["## Install Stylegan2-ada-pytorch Prerequisites"],"metadata":{"id":"tntmbryUFuHb"}},{"cell_type":"code","source":["%tensorflow_version 1.x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tc5AWKnMo0DH","executionInfo":{"status":"ok","timestamp":1658975375010,"user_tz":-540,"elapsed":4,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"8fd98959-2e93-40c0-b46d-ad8e1ad282fa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING: Tensorflow 1 is deprecated, and support will be removed on August 1, 2022.\n","After that, `%tensorflow_version 1.x` will throw an error.\n","\n","Your notebook should be updated to use Tensorflow 2.\n","See the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.\n","\n","TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"AtOIUdtJo2H1","executionInfo":{"status":"ok","timestamp":1658975408465,"user_tz":-540,"elapsed":4053,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"af88648f-ccbc-4f2e-98db-f3a37acc1fd9"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.15.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch\n","import torchvision\n","\n","print(torch.cuda.device_count())\n","\n","print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"skXSuPOnF1Pg","executionInfo":{"status":"ok","timestamp":1658975021522,"user_tz":-540,"elapsed":6,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"f4d66020-6a73-4c90-e50b-6e8af747147f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","1.7.1\n"]}]},{"cell_type":"code","source":["!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KwyG0BtF7nr","executionInfo":{"status":"ok","timestamp":1658974588976,"user_tz":-540,"elapsed":6366,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"1181598b-54a2-4739-f253-cf496878f191"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n","Collecting pyspng\n","  Downloading pyspng-0.1.0-cp37-cp37m-manylinux2010_x86_64.whl (195 kB)\n","\u001b[K     |████████████████████████████████| 195 kB 5.1 MB/s \n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n","\u001b[K     |████████████████████████████████| 108 kB 66.4 MB/s \n","\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n","  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[K     |████████████████████████████████| 26.9 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyspng) (1.21.6)\n","Installing collected packages: pyspng, ninja, imageio-ffmpeg\n","Successfully installed imageio-ffmpeg-0.4.3 ninja-1.10.2.3 pyspng-0.1.0\n"]}]},{"cell_type":"code","source":["!pip uninstall torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eq9fWUt8G-tO","executionInfo":{"status":"ok","timestamp":1658974652763,"user_tz":-540,"elapsed":62172,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"2e1b6dd4-8020-45ae-9f57-e04806de9491"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 1.12.0+cu113\n","Uninstalling torch-1.12.0+cu113:\n","  Would remove:\n","    /usr/local/bin/convert-caffe2-to-onnx\n","    /usr/local/bin/convert-onnx-to-caffe2\n","    /usr/local/bin/torchrun\n","    /usr/local/lib/python3.7/dist-packages/caffe2/*\n","    /usr/local/lib/python3.7/dist-packages/torch-1.12.0+cu113.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/torch/*\n","    /usr/local/lib/python3.7/dist-packages/torchgen/*\n","Proceed (y/n)? y\n","  Successfully uninstalled torch-1.12.0+cu113\n","Found existing installation: torchvision 0.13.0+cu113\n","Uninstalling torchvision-0.13.0+cu113:\n","  Would remove:\n","    /usr/local/lib/python3.7/dist-packages/torchvision-0.13.0+cu113.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libcudart.053364c0.so.11.0\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libjpeg.ceea7512.so.62\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libnvjpeg.90286a3c.so.11\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libpng16.7f72a3c5.so.16\n","    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libz.1328edc3.so.1\n","    /usr/local/lib/python3.7/dist-packages/torchvision/*\n","Proceed (y/n)? y\n","  Successfully uninstalled torchvision-0.13.0+cu113\n"]}]},{"cell_type":"code","source":["!pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"5CxCjW_-LWwc","executionInfo":{"status":"ok","timestamp":1658974732665,"user_tz":-540,"elapsed":74434,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"bbe75edd-1d43-4d28-8a0d-cd960d1e7346"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.7.1\n","  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n","\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n","\u001b[?25hCollecting torchvision==0.8.2\n","  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 26.8 MB/s \n","\u001b[?25hCollecting torchaudio==0.7.2\n","  Downloading torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.21.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2) (7.1.2)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.12.0+cu113\n","    Uninstalling torchaudio-0.12.0+cu113:\n","      Successfully uninstalled torchaudio-0.12.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n","Successfully installed torch-1.7.1 torchaudio-0.7.2 torchvision-0.8.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchvision"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"1xchK742BaMf"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/StyleGAN2-ADA-pytorch/stylegan2-ada-pytorch\n","!python dataset_tool.py --source=/content/drive/MyDrive/StyleGAN2-ADA-pytorch/datasets/pokemon_jpg/ --dest=/content/drive/MyDrive/StyleGAN2-ADA-pytorch/datasets/pokemon_256_dataset.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtsvRgePHV0y","executionInfo":{"status":"ok","timestamp":1658974784567,"user_tz":-540,"elapsed":16833,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"e1393e0f-4f6d-403b-aa5f-31080188c62d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/StyleGAN2-ADA-pytorch/stylegan2-ada-pytorch\n","100% 819/819 [00:13<00:00, 62.51it/s] \n"]}]},{"cell_type":"markdown","source":["## Transfer learning"],"metadata":{"id":"P-AVhQKEIbqe"}},{"cell_type":"code","source":["!python train.py --help"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6ZqnaJKIsmZ","executionInfo":{"status":"ok","timestamp":1658974802998,"user_tz":-540,"elapsed":7048,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"9b53733d-bd3c-4469-b9c9-906b3d8c708a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Usage: train.py [OPTIONS]\n","\n","  Train a GAN using the techniques described in\n","  the paper \"Training Generative Adversarial\n","  Networks with Limited Data\".\n","\n","  Examples:\n","\n","  # Train with custom dataset using 1 GPU.\n","  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n","\n","  # Train class-conditional CIFAR-10 using 2 GPUs.\n","  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\n","      --gpus=2 --cfg=cifar --cond=1\n","\n","  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n","  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\n","      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n","\n","  # Reproduce original StyleGAN2 config F.\n","  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\n","      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n","\n","  Base configs (--cfg):\n","    auto       Automatically select reasonable defaults based on resolution\n","               and GPU count. Good starting point for new datasets.\n","    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n","    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n","    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n","    paper1024  Reproduce results for MetFaces at 1024x1024.\n","    cifar      Reproduce results for CIFAR-10 at 32x32.\n","\n","  Transfer learning source networks (--resume):\n","    ffhq256        FFHQ trained at 256x256 resolution.\n","    ffhq512        FFHQ trained at 512x512 resolution.\n","    ffhq1024       FFHQ trained at 1024x1024 resolution.\n","    celebahq256    CelebA-HQ trained at 256x256 resolution.\n","    lsundog256     LSUN Dog trained at 256x256 resolution.\n","    <PATH or URL>  Custom network pickle.\n","\n","Options:\n","  --outdir DIR                    Where to save\n","                                  the results\n","                                  [required]\n","\n","  --gpus INT                      Number of GPUs\n","                                  to use [default:\n","                                  1]\n","\n","  --snap INT                      Snapshot\n","                                  interval\n","                                  [default: 50\n","                                  ticks]\n","\n","  --metrics LIST                  Comma-separated\n","                                  list or \"none\"\n","                                  [default:\n","                                  fid50k_full]\n","\n","  --seed INT                      Random seed\n","                                  [default: 0]\n","\n","  -n, --dry-run                   Print training\n","                                  options and exit\n","\n","  --data PATH                     Training data\n","                                  (directory or\n","                                  zip)  [required]\n","\n","  --cond BOOL                     Train\n","                                  conditional\n","                                  model based on\n","                                  dataset labels\n","                                  [default: false]\n","\n","  --subset INT                    Train with only\n","                                  N images\n","                                  [default: all]\n","\n","  --mirror BOOL                   Enable dataset\n","                                  x-flips\n","                                  [default: false]\n","\n","  --cfg [auto|stylegan2|paper256|paper512|paper1024|cifar]\n","                                  Base config\n","                                  [default: auto]\n","\n","  --gamma FLOAT                   Override R1\n","                                  gamma\n","\n","  --kimg INT                      Override\n","                                  training\n","                                  duration\n","\n","  --batch INT                     Override batch\n","                                  size\n","\n","  --aug [noaug|ada|fixed]         Augmentation\n","                                  mode [default:\n","                                  ada]\n","\n","  --p FLOAT                       Augmentation\n","                                  probability for\n","                                  --aug=fixed\n","\n","  --target FLOAT                  ADA target value\n","                                  for --aug=ada\n","\n","  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\n","                                  Augmentation\n","                                  pipeline\n","                                  [default: bgc]\n","\n","  --resume PKL                    Resume training\n","                                  [default:\n","                                  noresume]\n","\n","  --freezed INT                   Freeze-D\n","                                  [default: 0\n","                                  layers]\n","\n","  --fp32 BOOL                     Disable mixed-\n","                                  precision\n","                                  training\n","\n","  --nhwc BOOL                     Use NHWC memory\n","                                  format with FP16\n","\n","  --nobench BOOL                  Disable cuDNN\n","                                  benchmarking\n","\n","  --allow-tf32 BOOL               Allow PyTorch to\n","                                  use TF32\n","                                  internally\n","\n","  --workers INT                   Override number\n","                                  of DataLoader\n","                                  workers\n","\n","  --help                          Show this\n","                                  message and\n","                                  exit.\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/StyleGAN2-ADA-pytorch/stylegan2-ada-pytorch\n","!python train.py --resume=lsundog256 \\\n","                 --outdir=/content/drive/MyDrive/StyleGAN2-ADA-pytorch/result \\\n","                 --data=/content/drive/MyDrive/StyleGAN2-ADA-pytorch/datasets/pokemon_256_dataset.zip \\\n","                 --gpus=1 \\\n","                 --cfg=paper256 \\\n","                 --mirror=1 \\\n","                 --snap=10 \\\n","                 --metrics=none"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4VX0wrTfHV8Y","outputId":"0b6205fd-8b14-4576-e4d1-cee655a2ea77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/StyleGAN2-ADA-pytorch/stylegan2-ada-pytorch\n","\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 10,\n","  \"network_snapshot_ticks\": 10,\n","  \"metrics\": [],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/drive/MyDrive/StyleGAN2-ADA-pytorch/datasets/pokemon_256_dataset.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 819,\n","    \"xflip\": true,\n","    \"resolution\": 256\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 8\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 16384,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 8\n","    },\n","    \"channel_base\": 16384,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 1\n","  },\n","  \"total_kimg\": 25000,\n","  \"batch_size\": 64,\n","  \"batch_gpu\": 8,\n","  \"ema_kimg\": 20,\n","  \"ema_rampup\": null,\n","  \"ada_target\": 0.6,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1,\n","    \"brightness\": 1,\n","    \"contrast\": 1,\n","    \"lumaflip\": 1,\n","    \"hue\": 1,\n","    \"saturation\": 1\n","  },\n","  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/lsundog-res256-paper256-kimg100000-noaug.pkl\",\n","  \"ada_kimg\": 100,\n","  \"run_dir\": \"/content/drive/MyDrive/StyleGAN2-ADA-pytorch/result/00000-pokemon_256_dataset-mirror-paper256-resumelsundog256\"\n","}\n","\n","Output directory:   /content/drive/MyDrive/StyleGAN2-ADA-pytorch/result/00000-pokemon_256_dataset-mirror-paper256-resumelsundog256\n","Training data:      /content/drive/MyDrive/StyleGAN2-ADA-pytorch/datasets/pokemon_256_dataset.zip\n","Training duration:  25000 kimg\n","Number of GPUs:     1\n","Number of images:   819\n","Image resolution:   256\n","Conditional model:  False\n","Dataset x-flips:    True\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","\n","Num images:  1638\n","Image shape: [3, 256, 256]\n","Label shape: [0]\n","\n","Constructing networks...\n","Resuming from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/lsundog-res256-paper256-kimg100000-noaug.pkl\"\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Generator             Parameters  Buffers  Output shape        Datatype\n","---                   ---         ---      ---                 ---     \n","mapping.fc0           262656      -        [8, 512]            float32 \n","mapping.fc1           262656      -        [8, 512]            float32 \n","mapping.fc2           262656      -        [8, 512]            float32 \n","mapping.fc3           262656      -        [8, 512]            float32 \n","mapping.fc4           262656      -        [8, 512]            float32 \n","mapping.fc5           262656      -        [8, 512]            float32 \n","mapping.fc6           262656      -        [8, 512]            float32 \n","mapping.fc7           262656      -        [8, 512]            float32 \n","mapping               -           512      [8, 14, 512]        float32 \n","synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n","synthesis.b4.torgb    264195      -        [8, 3, 4, 4]        float32 \n","synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n","synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n","synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n","synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n","synthesis.b8.torgb    264195      -        [8, 3, 8, 8]        float32 \n","synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n","synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n","synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n","synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n","synthesis.b16.torgb   264195      -        [8, 3, 16, 16]      float32 \n","synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n","synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n","synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float16 \n","synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float16 \n","synthesis.b32.torgb   264195      -        [8, 3, 32, 32]      float16 \n","synthesis.b32:0       -           16       [8, 512, 32, 32]    float16 \n","synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n","synthesis.b64.conv0   1442561     4112     [8, 256, 64, 64]    float16 \n","synthesis.b64.conv1   721409      4112     [8, 256, 64, 64]    float16 \n","synthesis.b64.torgb   132099      -        [8, 3, 64, 64]      float16 \n","synthesis.b64:0       -           16       [8, 256, 64, 64]    float16 \n","synthesis.b64:1       -           -        [8, 256, 64, 64]    float32 \n","synthesis.b128.conv0  426369      16400    [8, 128, 128, 128]  float16 \n","synthesis.b128.conv1  213249      16400    [8, 128, 128, 128]  float16 \n","synthesis.b128.torgb  66051       -        [8, 3, 128, 128]    float16 \n","synthesis.b128:0      -           16       [8, 128, 128, 128]  float16 \n","synthesis.b128:1      -           -        [8, 128, 128, 128]  float32 \n","synthesis.b256.conv0  139457      65552    [8, 64, 256, 256]   float16 \n","synthesis.b256.conv1  69761       65552    [8, 64, 256, 256]   float16 \n","synthesis.b256.torgb  33027       -        [8, 3, 256, 256]    float16 \n","synthesis.b256:0      -           16       [8, 64, 256, 256]   float16 \n","synthesis.b256:1      -           -        [8, 64, 256, 256]   float32 \n","---                   ---         ---      ---                 ---     \n","Total                 24767458    175568   -                   -       \n","\n","\n","Discriminator  Parameters  Buffers  Output shape        Datatype\n","---            ---         ---      ---                 ---     \n","b256.fromrgb   256         16       [8, 64, 256, 256]   float16 \n","b256.skip      8192        16       [8, 128, 128, 128]  float16 \n","b256.conv0     36928       16       [8, 64, 256, 256]   float16 \n","b256.conv1     73856       16       [8, 128, 128, 128]  float16 \n","b256           -           16       [8, 128, 128, 128]  float16 \n","b128.skip      32768       16       [8, 256, 64, 64]    float16 \n","b128.conv0     147584      16       [8, 128, 128, 128]  float16 \n","b128.conv1     295168      16       [8, 256, 64, 64]    float16 \n","b128           -           16       [8, 256, 64, 64]    float16 \n","b64.skip       131072      16       [8, 512, 32, 32]    float16 \n","b64.conv0      590080      16       [8, 256, 64, 64]    float16 \n","b64.conv1      1180160     16       [8, 512, 32, 32]    float16 \n","b64            -           16       [8, 512, 32, 32]    float16 \n","b32.skip       262144      16       [8, 512, 16, 16]    float16 \n","b32.conv0      2359808     16       [8, 512, 32, 32]    float16 \n","b32.conv1      2359808     16       [8, 512, 16, 16]    float16 \n","b32            -           16       [8, 512, 16, 16]    float16 \n","b16.skip       262144      16       [8, 512, 8, 8]      float32 \n","b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n","b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n","b16            -           16       [8, 512, 8, 8]      float32 \n","b8.skip        262144      16       [8, 512, 4, 4]      float32 \n","b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n","b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n","b8             -           16       [8, 512, 4, 4]      float32 \n","b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n","b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n","b4.fc          4194816     -        [8, 512]            float32 \n","b4.out         513         -        [8, 1]              float32 \n","---            ---         ---      ---                 ---     \n","Total          24001089    416      -                   -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Initializing logs...\n","Training for 25000 kimg...\n","\n","tick 0     kimg 0.1      time 49s          sec/tick 13.2    sec/kimg 206.50  maintenance 36.2   cpumem 5.22   gpumem 10.66  augment 0.000\n","tick 1     kimg 4.1      time 5m 59s       sec/tick 291.8   sec/kimg 72.38   maintenance 17.4   cpumem 5.30   gpumem 2.97   augment 0.036\n","tick 2     kimg 8.1      time 10m 55s      sec/tick 296.8   sec/kimg 73.60   maintenance 0.0    cpumem 5.30   gpumem 2.97   augment 0.064\n","tick 3     kimg 12.2     time 15m 53s      sec/tick 297.1   sec/kimg 73.69   maintenance 0.1    cpumem 5.30   gpumem 2.99   augment 0.090\n","tick 4     kimg 16.2     time 20m 50s      sec/tick 297.3   sec/kimg 73.74   maintenance 0.1    cpumem 5.30   gpumem 2.99   augment 0.110\n","tick 5     kimg 20.2     time 25m 46s      sec/tick 295.7   sec/kimg 73.35   maintenance 0.1    cpumem 5.30   gpumem 2.99   augment 0.136\n","tick 6     kimg 24.3     time 30m 44s      sec/tick 297.9   sec/kimg 73.87   maintenance 0.0    cpumem 5.30   gpumem 2.98   augment 0.169\n","tick 7     kimg 28.3     time 35m 42s      sec/tick 297.9   sec/kimg 73.88   maintenance 0.1    cpumem 5.30   gpumem 2.99   augment 0.184\n","tick 8     kimg 32.3     time 40m 40s      sec/tick 297.9   sec/kimg 73.88   maintenance 0.1    cpumem 5.30   gpumem 3.00   augment 0.215\n","tick 9     kimg 36.4     time 45m 36s      sec/tick 296.5   sec/kimg 73.53   maintenance 0.1    cpumem 5.30   gpumem 3.00   augment 0.251\n","tick 10    kimg 40.4     time 50m 35s      sec/tick 298.5   sec/kimg 74.02   maintenance 0.0    cpumem 5.30   gpumem 3.00   augment 0.274\n","tick 11    kimg 44.4     time 55m 52s      sec/tick 298.8   sec/kimg 74.11   maintenance 18.6   cpumem 5.30   gpumem 2.99   augment 0.310\n","tick 12    kimg 48.4     time 1h 00m 51s   sec/tick 298.6   sec/kimg 74.07   maintenance 0.1    cpumem 5.30   gpumem 3.00   augment 0.346\n","tick 13    kimg 52.5     time 1h 05m 48s   sec/tick 297.0   sec/kimg 73.66   maintenance 0.1    cpumem 5.30   gpumem 3.01   augment 0.371\n","tick 14    kimg 56.5     time 1h 10m 47s   sec/tick 298.7   sec/kimg 74.08   maintenance 0.0    cpumem 5.30   gpumem 3.03   augment 0.389\n","tick 15    kimg 60.5     time 1h 15m 46s   sec/tick 299.0   sec/kimg 74.15   maintenance 0.1    cpumem 5.30   gpumem 3.06   augment 0.399\n","tick 16    kimg 64.6     time 1h 20m 45s   sec/tick 298.9   sec/kimg 74.13   maintenance 0.1    cpumem 5.30   gpumem 3.02   augment 0.415\n","tick 17    kimg 68.6     time 1h 25m 40s   sec/tick 295.7   sec/kimg 73.35   maintenance 0.1    cpumem 5.30   gpumem 3.00   augment 0.435\n","tick 18    kimg 72.6     time 1h 30m 40s   sec/tick 299.1   sec/kimg 74.19   maintenance 0.0    cpumem 5.30   gpumem 3.01   augment 0.458\n","tick 19    kimg 76.7     time 1h 35m 39s   sec/tick 299.2   sec/kimg 74.21   maintenance 0.1    cpumem 5.30   gpumem 3.01   augment 0.484\n","tick 20    kimg 80.7     time 1h 40m 38s   sec/tick 299.4   sec/kimg 74.24   maintenance 0.1    cpumem 5.30   gpumem 3.02   augment 0.499\n","tick 21    kimg 84.7     time 1h 45m 54s   sec/tick 298.2   sec/kimg 73.95   maintenance 17.1   cpumem 5.23   gpumem 3.02   augment 0.520\n","tick 22    kimg 88.8     time 1h 50m 53s   sec/tick 299.4   sec/kimg 74.25   maintenance 0.0    cpumem 5.23   gpumem 3.03   augment 0.538\n"]}]},{"cell_type":"code","source":["# dataset_name = \"pokemon-256\""],"metadata":{"id":"fwsaVC73BRzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 파일이 압축파일일 경우에만 실행한다.\n","\n","# import zipfile\n","# path = \"/content/drive/MyDrive/StyleGAN2-ADA/datasets/\"\n","# dataset = dataset_name + \".zip\"\n","# local_path = \"/content/drive/MyDrive/StyleGAN2-ADA/datasets/\"\n","# file_name = path + dataset\n","# with zipfile.ZipFile(file_name, 'r') as zip:\n","#    #zip.printdir()\n","#    print('Extracting all the files now...') \n","#    zip.extractall(local_path)\n","#    print('Done!')"],"metadata":{"id":"yVzBafYNBR3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #update this to the path to your image folder\n","# dataset_folder_name = 'pokemon_jpg/pokemon_jpg' # name of zip file may be different from folder name it extracted to\n","# dataset_path = \"/content/drive/MyDrive/StyleGAN2-ADA/datasets/\" + dataset_folder_name\n","\n","# #you don't need to edit anything here\n","# %cd /content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada\n","# !python dataset_tool.py create_from_images /content/drive/MyDrive/StyleGAN2-ADA/datasets/{dataset_name} {dataset_path}"],"metadata":{"id":"-ni_px7UBR5s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"FIMztNjmBjgV"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/StyleGAN2-ADA-pytorch/stylegan2-ada-pytorch/\n","!python train.py --help"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_riPg8jBR7-","executionInfo":{"status":"ok","timestamp":1658747529999,"user_tz":-540,"elapsed":4601,"user":{"displayName":"김22","userId":"17014669673010972137"}},"outputId":"048b002c-6caa-4c36-9ea0-a71c5fa2e107"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/StyleGAN2-ADA-pytorch/stylegan2-ada-pytorch\n","Usage: train.py [OPTIONS]\n","\n","  Train a GAN using the techniques described in\n","  the paper \"Training Generative Adversarial\n","  Networks with Limited Data\".\n","\n","  Examples:\n","\n","  # Train with custom dataset using 1 GPU.\n","  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n","\n","  # Train class-conditional CIFAR-10 using 2 GPUs.\n","  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\n","      --gpus=2 --cfg=cifar --cond=1\n","\n","  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n","  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\n","      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n","\n","  # Reproduce original StyleGAN2 config F.\n","  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\n","      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n","\n","  Base configs (--cfg):\n","    auto       Automatically select reasonable defaults based on resolution\n","               and GPU count. Good starting point for new datasets.\n","    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n","    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n","    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n","    paper1024  Reproduce results for MetFaces at 1024x1024.\n","    cifar      Reproduce results for CIFAR-10 at 32x32.\n","\n","  Transfer learning source networks (--resume):\n","    ffhq256        FFHQ trained at 256x256 resolution.\n","    ffhq512        FFHQ trained at 512x512 resolution.\n","    ffhq1024       FFHQ trained at 1024x1024 resolution.\n","    celebahq256    CelebA-HQ trained at 256x256 resolution.\n","    lsundog256     LSUN Dog trained at 256x256 resolution.\n","    <PATH or URL>  Custom network pickle.\n","\n","Options:\n","  --outdir DIR                    Where to save\n","                                  the results\n","                                  [required]\n","\n","  --gpus INT                      Number of GPUs\n","                                  to use [default:\n","                                  1]\n","\n","  --snap INT                      Snapshot\n","                                  interval\n","                                  [default: 50\n","                                  ticks]\n","\n","  --metrics LIST                  Comma-separated\n","                                  list or \"none\"\n","                                  [default:\n","                                  fid50k_full]\n","\n","  --seed INT                      Random seed\n","                                  [default: 0]\n","\n","  -n, --dry-run                   Print training\n","                                  options and exit\n","\n","  --data PATH                     Training data\n","                                  (directory or\n","                                  zip)  [required]\n","\n","  --cond BOOL                     Train\n","                                  conditional\n","                                  model based on\n","                                  dataset labels\n","                                  [default: false]\n","\n","  --subset INT                    Train with only\n","                                  N images\n","                                  [default: all]\n","\n","  --mirror BOOL                   Enable dataset\n","                                  x-flips\n","                                  [default: false]\n","\n","  --cfg [auto|stylegan2|paper256|paper512|paper1024|cifar]\n","                                  Base config\n","                                  [default: auto]\n","\n","  --gamma FLOAT                   Override R1\n","                                  gamma\n","\n","  --kimg INT                      Override\n","                                  training\n","                                  duration\n","\n","  --batch INT                     Override batch\n","                                  size\n","\n","  --aug [noaug|ada|fixed]         Augmentation\n","                                  mode [default:\n","                                  ada]\n","\n","  --p FLOAT                       Augmentation\n","                                  probability for\n","                                  --aug=fixed\n","\n","  --target FLOAT                  ADA target value\n","                                  for --aug=ada\n","\n","  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\n","                                  Augmentation\n","                                  pipeline\n","                                  [default: bgc]\n","\n","  --resume PKL                    Resume training\n","                                  [default:\n","                                  noresume]\n","\n","  --freezed INT                   Freeze-D\n","                                  [default: 0\n","                                  layers]\n","\n","  --fp32 BOOL                     Disable mixed-\n","                                  precision\n","                                  training\n","\n","  --nhwc BOOL                     Use NHWC memory\n","                                  format with FP16\n","\n","  --nobench BOOL                  Disable cuDNN\n","                                  benchmarking\n","\n","  --allow-tf32 BOOL               Allow PyTorch to\n","                                  use TF32\n","                                  internally\n","\n","  --workers INT                   Override number\n","                                  of DataLoader\n","                                  workers\n","\n","  --help                          Show this\n","                                  message and\n","                                  exit.\n"]}]},{"cell_type":"code","source":["# dataset path\n","\n","\n","# output directory\n","output_dir = '/content/drive/MyDrive/StyleGAN2-ADA-pytorch/results/' + dataset_name + \"/\"\n","\n","# config\n","config = \"auto\"\n","\n","resume_from = \"lsundog256\"\n","\n","# make sure there is no space in the resume path. if there is any, use a backslash character to escape\n","!python train.py --outdir={output_dir} \\\n","                 --data=~/mydataset.zip \\\n","                 --gpus=1\n","\n","\n","#don't edit this unless you know what you're doing :)\n","# !python train.py --outdir={output_dir} \\\n","#                  --gpus=1 \\\n","#                  --cfg={config} \\\n","#                  --snap={snapshot_count} \\\n","#                  --data=/content/drive/MyDrive/StyleGAN2-ADA/datasets/{dataset_name} \\\n","#                  --mirror={mirrored} --mirrory={mirroredY} \\\n","#                  --gamma={gamma} \\\n","#                  --metrics={metric_list} \\\n","#                  --resume={resume_from}\n","\n","\n","\n","\n","# # #how often should the model generate samples and a .pkl file\n","# # snapshot_count = 4\n","# # #should the images be mirrored left to right?\n","# # mirrored = True\n","# # #should the images be mirrored top to bottom?\n","# # mirroredY = True\n","# # #metrics? \n","# # metric_list = None\n","# # #augments\n","# augs = \"bg\"\n","\n","# # #\n","# # # this is the most important cell to update\n","# # #\n","# # # running it for the first time? set it to ffhq(+resolution)\n","# # # resuming? get the path to your latest .pkl file and use that\n","# # resume_from = \"lsundog256\"\n","\n","# #don't edit this unless you know what you're doing :)\n","# !python train.py --outdir={output_dir} \\\n","#                  --snap={snapshot_count} \\\n","#                  --cfg=11gb-gpu \\\n","#                  --data=/content/drive/MyDrive/StyleGAN2-ADA/datasets/{dataset_name} \\\n","#                  --augpipe={augs} \\\n","#                  --mirror={mirrored} \\\n","#                  --mirrory={mirroredY} \\\n","#                  --metrics={metric_list} \\\n","#                  --resume={resume_from} \\\n","#                  --augpipe=\"bg\""],"metadata":{"id":"mzuHbr5SBR92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"h50PXZVmBSCk"},"execution_count":null,"outputs":[]}]}